{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSmPIe4gN71qXdlqE9ddrE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wenqiglantz/devsecops-end-to-end-eval/blob/main/recursive_doc_agents_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End-to-End Evaluation for RAG Pipeline with Recursive Document Agents\n",
        "\n",
        "Let's evaluate this RAG pipeline for DevSecOps which was implemented with recursive document agents."
      ],
      "metadata": {
        "id": "T8Y-rApSQOta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the query engine"
      ],
      "metadata": {
        "id": "S3AIawHlUo5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install LlamaIndex and set up"
      ],
      "metadata": {
        "id": "V-klMHFlUqGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index==0.8.13\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EeHhmD5wFB7",
        "outputId": "76a4be03-40e7-4093-a11c-57546fc04824"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama_index==0.8.13\n",
            "  Downloading llama_index-0.8.13-py3-none-any.whl (736 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m736.5/736.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.13) (0.4.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.13) (0.5.14)\n",
            "Requirement already satisfied: langchain>=0.0.262 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.13) (0.0.276)\n",
            "Requirement already satisfied: sqlalchemy>=2.0.15 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.13) (2.0.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.13) (1.23.5)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.13) (8.2.3)\n",
            "Requirement already satisfied: openai>=0.26.4 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.13) (0.27.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.13) (1.5.3)\n",
            "Requirement already satisfied: urllib3<2 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.13) (1.26.16)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.13) (2023.6.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.13) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.13) (4.7.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.13) (4.11.2)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.13) (1.5.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index==0.8.13) (6.0.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index==0.8.13) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index==0.8.13) (4.0.3)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index==0.8.13) (0.0.28)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index==0.8.13) (2.8.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index==0.8.13) (2.2.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index==0.8.13) (2.31.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama_index==0.8.13) (3.20.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai>=0.26.4->llama_index==0.8.13) (4.66.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=2.0.15->llama_index==0.8.13) (2.0.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama_index==0.8.13) (1.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->llama_index==0.8.13) (2.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index==0.8.13) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index==0.8.13) (2023.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->llama_index==0.8.13) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama_index==0.8.13) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama_index==0.8.13) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama_index==0.8.13) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama_index==0.8.13) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama_index==0.8.13) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama_index==0.8.13) (1.3.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama_index==0.8.13) (23.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain>=0.0.262->llama_index==0.8.13) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain>=0.0.262->llama_index==0.8.13) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama_index==0.8.13) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.262->llama_index==0.8.13) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.262->llama_index==0.8.13) (2023.7.22)\n",
            "Installing collected packages: llama_index\n",
            "  Attempting uninstall: llama_index\n",
            "    Found existing installation: llama-index 0.8.12\n",
            "    Uninstalling llama-index-0.8.12:\n",
            "      Successfully uninstalled llama-index-0.8.12\n",
            "Successfully installed llama_index-0.8.13\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.15.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EWFRhJpBvZQc"
      },
      "outputs": [],
      "source": [
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    ListIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        "    Response\n",
        ")\n",
        "from llama_index.evaluation import (\n",
        "    DatasetGenerator,\n",
        "    QueryResponseEvaluator,\n",
        "    ResponseEvaluator\n",
        ")\n",
        "from llama_index.retrievers import RecursiveRetriever\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "from llama_index.response_synthesizers import get_response_synthesizer\n",
        "from llama_index.schema import IndexNode\n",
        "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.agent import OpenAIAgent\n",
        "import pandas as pd\n",
        "import openai\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = 'YOUR-API-KEY'\n",
        "\n",
        "#define LLM\n",
        "llm = OpenAI(temperature=0.1, model_name=\"gpt-3.5-turbo\")\n",
        "service_context = ServiceContext.from_defaults(llm=llm)"
      ],
      "metadata": {
        "id": "NhCJLikmvja4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63bb328b-7ab5-472c-c89f-299c231173a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtzmoF_a3Rvb",
        "outputId": "110cb552-8a19-4797-9cb9-45334b03ae4c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load documents"
      ],
      "metadata": {
        "id": "yMGL3DuuUyBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titles = [\n",
        "    \"DevOps Self-Service Pipeline Architecture and Its 3–2–1 Rule\",\n",
        "    \"DevOps Self-Service Centric Terraform Project Structure\",\n",
        "    \"DevOps Self-Service Centric Pipeline Security and Guardrails\"\n",
        "    ]\n",
        "\n",
        "documents = {}\n",
        "for title in titles:\n",
        "    documents[title] = SimpleDirectoryReader(input_files=[f\"data/{title}.pdf\"]).load_data()\n",
        "print(f\"loaded documents with {len(documents)} documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHsubrwxvsUB",
        "outputId": "a4bcc6ba-a774-4bfc-ab14-2a0d488aff6e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded documents with 3 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build document agents"
      ],
      "metadata": {
        "id": "Wezg2decwuwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build agents dictionary\n",
        "agents = {}\n",
        "\n",
        "for title in titles:\n",
        "\n",
        "    # build vector index\n",
        "    vector_index = VectorStoreIndex.from_documents(documents[title], service_context=service_context)\n",
        "\n",
        "    # build list index\n",
        "    list_index = ListIndex.from_documents(documents[title], service_context=service_context)\n",
        "\n",
        "    # define query engines\n",
        "    vector_query_engine = vector_index.as_query_engine()\n",
        "    list_query_engine = list_index.as_query_engine()\n",
        "\n",
        "    # define tools\n",
        "    query_engine_tools = [\n",
        "        QueryEngineTool(\n",
        "            query_engine=vector_query_engine,\n",
        "            metadata=ToolMetadata(\n",
        "                name=\"vector_tool\",\n",
        "                description=f\"Useful for retrieving specific context related to {title}\",\n",
        "            ),\n",
        "        ),\n",
        "        QueryEngineTool(\n",
        "            query_engine=list_query_engine,\n",
        "            metadata=ToolMetadata(\n",
        "                name=\"summary_tool\",\n",
        "                description=f\"Useful for summarization questions related to {title}\",\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    # build agent\n",
        "    function_llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
        "    agent = OpenAIAgent.from_tools(\n",
        "        query_engine_tools,\n",
        "        llm=function_llm,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    agents[title] = agent\n"
      ],
      "metadata": {
        "id": "48wW2HE0vzjS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build index nodes, indexes, and query engine"
      ],
      "metadata": {
        "id": "k346mAliVNne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define index nodes that link to the document agents\n",
        "nodes = []\n",
        "for title in titles:\n",
        "    doc_summary = (\n",
        "        f\"This content contains details about {title}. \"\n",
        "        f\"Use this index if you need to lookup specific facts about {title}.\\n\"\n",
        "        \"Do not use this index if you want to query multiple documents.\"\n",
        "    )\n",
        "    node = IndexNode(text=doc_summary, index_id=title)\n",
        "    nodes.append(node)\n",
        "\n",
        "# define retriever\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "vector_retriever = vector_index.as_retriever(similarity_top_k=1)\n",
        "\n",
        "# define recursive retriever\n",
        "# note: can pass `agents` dict as `query_engine_dict` since every agent can be used as a query engine\n",
        "recursive_retriever = RecursiveRetriever(\n",
        "    \"vector\",\n",
        "    retriever_dict={\"vector\": vector_retriever},\n",
        "    query_engine_dict=agents,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "response_synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
        "\n",
        "# define query engine\n",
        "query_engine = RetrieverQueryEngine.from_args(\n",
        "    recursive_retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        "    service_context=service_context,\n",
        ")\n"
      ],
      "metadata": {
        "id": "QeLU8cDkv2XR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End-to-End Evaluation"
      ],
      "metadata": {
        "id": "eJ5RNlJdVeJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate the dataset"
      ],
      "metadata": {
        "id": "roXsXlNNVjwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "from llama_index.prompts import Prompt\n",
        "\n",
        "# load data\n",
        "document_list = SimpleDirectoryReader(\"data\").load_data()\n",
        "\n",
        "# use gpt-4 to evaluate\n",
        "gpt4_service_context = ServiceContext.from_defaults(llm=OpenAI(temperature=0.1, llm=\"gpt-4\"))\n",
        "\n",
        "question_dataset = []\n",
        "if os.path.exists(\"question_dataset.txt\"):\n",
        "    with open(\"question_dataset.txt\", \"r\") as f:\n",
        "        for line in f:\n",
        "            question_dataset.append(line.strip())\n",
        "else:\n",
        "    # generate questions\n",
        "    data_generator = DatasetGenerator.from_documents(\n",
        "        document_list,\n",
        "        text_question_template=Prompt(\n",
        "            \"A sample from the documents is below.\\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"{context_str}\\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"Using the documentation sample, carefully follow the instructions below:\\n\"\n",
        "            \"{query_str}\"\n",
        "        ),\n",
        "        question_gen_query=(\n",
        "            \"You are an evaluator for a search pipeline. Your task is to write a list of summarization \"\n",
        "            \"questions or question/answer questions using the provided documents. Restrict the questions to the \"\n",
        "            \"context information provided.\\n\"\n",
        "            \"Question: \"\n",
        "        ),\n",
        "        service_context=gpt4_service_context\n",
        "    )\n",
        "    generated_questions = data_generator.generate_questions_from_nodes()\n",
        "    print(f\"Generated {len(generated_questions)} questions.\")\n",
        "\n",
        "    # randomly pick 30 questions\n",
        "    generated_questions = random.sample(generated_questions, 30)\n",
        "    question_dataset.extend(generated_questions)\n",
        "    print(f\"Randomly picked {len(question_dataset)} questions.\")\n",
        "\n",
        "    # save the questions into a txt file\n",
        "    with open(\"question_dataset.txt\", \"w\") as f:\n",
        "        for question in question_dataset:\n",
        "            f.write(f\"{question.strip()}\\n\")"
      ],
      "metadata": {
        "id": "7LKgp0LhvvpD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print the questions"
      ],
      "metadata": {
        "id": "ddQyLnhCVqH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, question in enumerate(question_dataset, start=1):\n",
        "    print(f\"{i}. {question}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arZvdpdNhnyx",
        "outputId": "c8b1336c-1bcd-4cd8-e87e-b183c7d7fa5b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. What is the high-level design of DevOps pipelines?\n",
            "2. What is a recently introduced feature in Infracost Cloud?\n",
            "3. What is the purpose of Infracost in cloud cost management?\n",
            "4. Why is it important to include TruffleHog in your pipelines?\n",
            "5. How can you fix the vulnerability in the base image according to the provided instructions?\n",
            "6. What is the purpose of the aquasecurity/trivy-action in the GitHub Actions CI workflow?\n",
            "7. What are the optional parameters that can be used with the Checkov action?\n",
            "8. How can Infracost be integrated into the infrastructure pipeline?\n",
            "9. How are application pipelines triggered?\n",
            "10. What is the topic of the second part in the series?\n",
            "11. What command is used to generate the Infracost report in HTML format?\n",
            "12. How does Terraform enable the creation of reusable infrastructure?\n",
            "13. How can the GitHub Actions workflow be configured to dynamically select the backend configuration file based on the environment?\n",
            "14. What is the diff feature in Infracost and how does it serve as a guardrail for cloud cost management?\n",
            "15. How does Infracost provide a safety net for catching abnormal cloud cost estimates?\n",
            "16. What is the purpose of uploading the report to an artifact?\n",
            "17. What types of files or systems can Trivy scan?\n",
            "18. What severity levels does Trivy consider for vulnerabilities?\n",
            "19. What is the purpose of the Terraform GitHub Actions workflow?\n",
            "20. Can you provide a link to a website that provides information on creating Terraform modules?\n",
            "21. What is the intended audience for these documents?\n",
            "22. What is the purpose of the \"DevOps Self-Service -Centric GitHub Actions’ Workflow Orchestration\" article?\n",
            "23. What is the purpose of the \"--soft-fail\" flag in the TFSec step of the workflow?\n",
            "24. What is the topic of the document?\n",
            "25. What command is used to initialize Terraform with a specific backend configuration file?\n",
            "26. What information can Trivy find during a scan?\n",
            "27. What CLI configuration files should be ignored?\n",
            "28. What is the purpose of Terraform as an Infrastructure as Code (IaC) tool?\n",
            "29. What is SonarScan and what does it analyze in source code?\n",
            "30. What are the three samples mentioned in the document?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define jupyter display function\n",
        "def display_eval_df(query: str, response: Response, eval_result: str) -> None:\n",
        "    eval_df = pd.DataFrame(\n",
        "        {\n",
        "            \"Query\": query,\n",
        "            \"Response\": str(response),\n",
        "            \"Source\": response.get_formatted_sources(500) + \"...\",\n",
        "            \"Evaluation Result\": eval_result,\n",
        "        },\n",
        "        index=[0],\n",
        "    )\n",
        "    eval_df = eval_df.style.set_properties(\n",
        "        **{\n",
        "            \"inline-size\": \"600px\",\n",
        "            \"overflow-wrap\": \"break-word\",\n",
        "        },\n",
        "        subset=[\"Response\", \"Source\"]\n",
        "    )\n",
        "    display(eval_df)"
      ],
      "metadata": {
        "id": "7uNjIUK3v7Hj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Response, test with one question first"
      ],
      "metadata": {
        "id": "tjjRalHLVwyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# call ResponseEvaluator to evaluate the responses\n",
        "evaluator = ResponseEvaluator(service_context=gpt4_service_context)\n",
        "response_vector = query_engine.query(question_dataset[0])\n",
        "eval_result = evaluator.evaluate(response_vector)\n",
        "\n",
        "# display the eval data frame\n",
        "pd.set_option(\"display.max_colwidth\", 0)\n",
        "display_eval_df(question_dataset[0], response_vector, eval_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "2A2gMhZOEhij",
        "outputId": "ca9fa827-0b27-45fe-f78a-3fff32d5a669"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x79f99f91b010>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_6b88b_row0_col1, #T_6b88b_row0_col2 {\n",
              "  inline-size: 600px;\n",
              "  overflow-wrap: break-word;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_6b88b\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_6b88b_level0_col0\" class=\"col_heading level0 col0\" >Query</th>\n",
              "      <th id=\"T_6b88b_level0_col1\" class=\"col_heading level0 col1\" >Response</th>\n",
              "      <th id=\"T_6b88b_level0_col2\" class=\"col_heading level0 col2\" >Source</th>\n",
              "      <th id=\"T_6b88b_level0_col3\" class=\"col_heading level0 col3\" >Evaluation Result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_6b88b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_6b88b_row0_col0\" class=\"data row0 col0\" >What is the high-level design of DevOps pipelines?</td>\n",
              "      <td id=\"T_6b88b_row0_col1\" class=\"data row0 col1\" >The high-level design of DevOps pipelines typically involves several stages, including source code management, build and compilation, automated testing, artifact repository, deployment, configuration management, continuous monitoring, feedback loop, and continuous improvement. This design aims to automate and streamline the software development and delivery process, enabling faster and more reliable releases while maintaining high quality.</td>\n",
              "      <td id=\"T_6b88b_row0_col2\" class=\"data row0 col2\" >> Source (Doc id: 2ae213ec-bc71-4651-b379-ac3b3f9f51c4): Query: What is the high-level design of DevOps pipelines?\n",
              "Response: The high-level design of DevOps pipelines typically follows a continuous integration and continuous delivery (CI/CD) approach. Here is a general overview of the high-level design:\n",
              "\n",
              "1. Source Code Management: Developers commit their code changes to a version control system (e.g., Git) to track and manage code changes.\n",
              "\n",
              "2. Build and Compilation: The pipeline triggers a build process to compile the source code and generate execu......</td>\n",
              "      <td id=\"T_6b88b_row0_col3\" class=\"data row0 col3\" >YES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Response for Hallucination"
      ],
      "metadata": {
        "id": "j_pOSbRwVyyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def evaluate_query_engine(evaluator, query_engine, questions):\n",
        "    async def run_query(query_engine, q):\n",
        "        try:\n",
        "            return await query_engine.aquery(q)\n",
        "        except:\n",
        "            return Response(response=\"Error, query failed.\")\n",
        "\n",
        "    total_correct = 0\n",
        "    all_results = []\n",
        "    for batch_size in range(0, len(questions), 5):\n",
        "        batch_qs = questions[batch_size:batch_size+5]\n",
        "\n",
        "        tasks = [run_query(query_engine, q) for q in batch_qs]\n",
        "        responses = asyncio.run(asyncio.gather(*tasks))\n",
        "        print(f\"finished batch {(batch_size // 5) + 1} out of {len(questions) // 5}\")\n",
        "\n",
        "        # eval for hallucination\n",
        "        if isinstance(evaluator, ResponseEvaluator):\n",
        "          for response in responses:\n",
        "              eval_result = 1 if \"YES\" in evaluator.evaluate(response) else 0\n",
        "              total_correct += eval_result\n",
        "              all_results.append(eval_result)\n",
        "        # eval for answer quality\n",
        "        elif isinstance(evaluator, QueryResponseEvaluator):\n",
        "          for question, response in zip(batch_qs, responses):\n",
        "              eval_result = 1 if \"YES\" in evaluator.evaluate(question, response) else 0\n",
        "              total_correct += eval_result\n",
        "              all_results.append(eval_result)\n",
        "\n",
        "        # helps avoid rate limits\n",
        "        time.sleep(1)\n",
        "\n",
        "    return total_correct, all_results"
      ],
      "metadata": {
        "id": "OmV5PGb4h352"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eval for hallucination\n",
        "total_correct, all_results = evaluate_query_engine(evaluator, query_engine, question_dataset)\n",
        "print(f\"Hallucination? Scored {total_correct} out of {len(question_dataset)} questions correctly.\")"
      ],
      "metadata": {
        "id": "Djkj0noBv9zC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e325709a-3203-4342-91e6-37b6a7f6f170"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished batch 1 out of 6\n",
            "finished batch 2 out of 6\n",
            "finished batch 3 out of 6\n",
            "finished batch 4 out of 6\n",
            "finished batch 5 out of 6\n",
            "finished batch 6 out of 6\n",
            "Hallucination? Scored 30 out of 30 questions correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find out the hallucinated questions and investigate why"
      ],
      "metadata": {
        "id": "oLjE-1TdV6_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "hallucinated_questions = np.array(question_dataset)[np.array(all_results) == 0]\n",
        "print(hallucinated_questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgj9cyq0E4eg",
        "outputId": "36e7546e-8d27-4b0e-eed7-5b8df60a27c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query('')\n",
        "print(str(response))\n",
        "print(\"-----------------\")\n",
        "print(response.get_formatted_sources(length=1000))"
      ],
      "metadata": {
        "id": "M3oBzk04rBJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Response for Answer Quality"
      ],
      "metadata": {
        "id": "qieav9vwWABs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = QueryResponseEvaluator(service_context=gpt4_service_context)\n",
        "\n",
        "total_correct, all_results = evaluate_query_engine(evaluator, query_engine, question_dataset)\n",
        "\n",
        "print(f\"Response satisfies the query? Scored {total_correct} out of {len(question_dataset)} questions correctly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMA1SA-wikT-",
        "outputId": "78bcef09-52ff-4ab5-9333-e6308a48af14"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished batch 1 out of 6\n",
            "finished batch 2 out of 6\n",
            "finished batch 3 out of 6\n",
            "finished batch 4 out of 6\n",
            "finished batch 5 out of 6\n",
            "finished batch 6 out of 6\n",
            "Response satisfies the query? Scored 29 out of 30 questions correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find out unanswered queries and investigate why"
      ],
      "metadata": {
        "id": "TFO3Y2x9WJd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "unanswered_queries = np.array(question_dataset)[np.array(all_results) == 0]\n",
        "print(unanswered_queries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7otBiDFioPt",
        "outputId": "0ce2caf6-e311-41ad-bbeb-d4bc0b9e9e40"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What is a recently introduced feature in Infracost Cloud?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query('What is a recently introduced feature in Infracost Cloud?')\n",
        "print(str(response))\n",
        "print(\"-----------------\")\n",
        "print(response.get_formatted_sources(length=256))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HccP7C6ZpWDd",
        "outputId": "9e2b6f1a-99fa-45b5-c224-19f6b4f5a7e5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but as an AI language model, I don't have access to real-time information. Therefore, I don't have information on the recently introduced features in Infracost Cloud. I recommend visiting the Infracost Cloud website or contacting their support team for the most up-to-date information on their features.\n",
            "-----------------\n",
            "> Source (Doc id: 3b240ae4-625f-4f6b-a2a0-23e90838876b): Query: What is a recently introduced feature in Infracost Cloud?\n",
            "Response: I'm sorry, but as an AI language model, I don't have access to real-time information. Therefore, I don't have information on the recently introduced features in Infracost Cloud. ...\n"
          ]
        }
      ]
    }
  ]
}