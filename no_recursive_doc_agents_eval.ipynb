{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# End-to-End Evaluation for RAG Pipeline without Recursive Document Agents\n",
        "\n",
        "Let's evaluate this RAG pipeline for DevSecOps which was implemented without recursive document agents."
      ],
      "metadata": {
        "id": "Vysv6Gc2Pfad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the query engine"
      ],
      "metadata": {
        "id": "7PmAitx3T5NS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install LlamaIndex and set up"
      ],
      "metadata": {
        "id": "ACu7WQPiT-5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index==0.8.12\n",
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EeHhmD5wFB7",
        "outputId": "c8c6a681-5a27-41b9-b246-a9907cb84268"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama_index==0.8.12 in /usr/local/lib/python3.10/dist-packages (0.8.12)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.12) (0.4.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.12) (0.5.14)\n",
            "Requirement already satisfied: langchain>=0.0.262 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.12) (0.0.275)\n",
            "Requirement already satisfied: sqlalchemy>=2.0.15 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.12) (2.0.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.12) (1.23.5)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.12) (8.2.3)\n",
            "Requirement already satisfied: openai>=0.26.4 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.12) (0.27.9)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.12) (1.5.3)\n",
            "Requirement already satisfied: urllib3<2 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.12) (1.26.16)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.12) (2023.6.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.12) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.12) (4.7.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.12) (4.11.2)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.12) (1.5.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index==0.8.12) (6.0.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index==0.8.12) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index==0.8.12) (4.0.3)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index==0.8.12) (0.0.27)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index==0.8.12) (2.8.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index==0.8.12) (2.2.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.262->llama_index==0.8.12) (2.31.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama_index==0.8.12) (3.20.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai>=0.26.4->llama_index==0.8.12) (4.66.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=2.0.15->llama_index==0.8.12) (2.0.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama_index==0.8.12) (1.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->llama_index==0.8.12) (2.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index==0.8.12) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index==0.8.12) (2023.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->llama_index==0.8.12) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama_index==0.8.12) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama_index==0.8.12) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama_index==0.8.12) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama_index==0.8.12) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama_index==0.8.12) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.262->llama_index==0.8.12) (1.3.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama_index==0.8.12) (23.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain>=0.0.262->llama_index==0.8.12) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain>=0.0.262->llama_index==0.8.12) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama_index==0.8.12) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.262->llama_index==0.8.12) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.262->llama_index==0.8.12) (2023.7.22)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.15.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "EWFRhJpBvZQc"
      },
      "outputs": [],
      "source": [
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        "    Response\n",
        ")\n",
        "from llama_index.evaluation import (\n",
        "    DatasetGenerator,\n",
        "    QueryResponseEvaluator,\n",
        "    ResponseEvaluator\n",
        ")\n",
        "from llama_index.llms import OpenAI\n",
        "import pandas as pd\n",
        "import openai\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = 'YOUR-API-KEY'\n",
        "\n",
        "#define LLM\n",
        "llm = OpenAI(temperature=0.1, model_name=\"gpt-3.5-turbo\")\n",
        "service_context = ServiceContext.from_defaults(llm=llm)"
      ],
      "metadata": {
        "id": "NhCJLikmvja4"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EH3a35lzgsP",
        "outputId": "9f5f0e2e-f4a7-4d0b-e4d3-dca1d20f96c7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2KguTM3ERs7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load documents, build index and query engine"
      ],
      "metadata": {
        "id": "Wezg2decwuwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load documents\n",
        "document_list = SimpleDirectoryReader(\"data\").load_data()\n",
        "\n",
        "# build vector index and query engine\n",
        "vector_index = VectorStoreIndex.from_documents(document_list, service_context=service_context)\n",
        "query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "id": "48wW2HE0vzjS"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "imIjOsO4UZSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End-to-End Evaluation"
      ],
      "metadata": {
        "id": "mzksd4cqSGP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate the dataset"
      ],
      "metadata": {
        "id": "QVV5vIuyUYmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "from llama_index.prompts import Prompt\n",
        "\n",
        "gpt4_service_context = ServiceContext.from_defaults(llm=OpenAI(temperature=0.1, llm=\"gpt-4\"))\n",
        "\n",
        "question_dataset = []\n",
        "if os.path.exists(\"question_dataset.txt\"):\n",
        "    with open(\"question_dataset.txt\", \"r\") as f:\n",
        "        for line in f:\n",
        "            question_dataset.append(line.strip())\n",
        "else:\n",
        "    # generate questions\n",
        "    data_generator = DatasetGenerator.from_documents(\n",
        "        document_list,\n",
        "        text_question_template=Prompt(\n",
        "            \"A sample from the documents is below.\\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"{context_str}\\n\"\n",
        "            \"---------------------\\n\"\n",
        "            \"Using the documentation sample, carefully follow the instructions below:\\n\"\n",
        "            \"{query_str}\"\n",
        "        ),\n",
        "        question_gen_query=(\n",
        "            \"You are an evaluator for a search pipeline. Your task is to write a list of summarization \"\n",
        "            \"questions or question/answer questions using the provided documents. Restrict the questions to the \"\n",
        "            \"context information provided.\\n\"\n",
        "            \"Question: \"\n",
        "        ),\n",
        "        # set this to be low, so we can generate more questions\n",
        "        service_context=gpt4_service_context\n",
        "    )\n",
        "    generated_questions = data_generator.generate_questions_from_nodes()\n",
        "    print(f\"Generated {len(generated_questions)} questions.\")\n",
        "\n",
        "    # randomly pick 30 questions from each dataset\n",
        "    generated_questions = random.sample(generated_questions, 30)\n",
        "    question_dataset.extend(generated_questions)\n",
        "\n",
        "    print(f\"Randomly picked {len(question_dataset)} questions.\")\n",
        "\n",
        "    # save the questions!\n",
        "    with open(\"question_dataset.txt\", \"w\") as f:\n",
        "        for question in question_dataset:\n",
        "            f.write(f\"{question.strip()}\\n\")"
      ],
      "metadata": {
        "id": "7LKgp0LhvvpD"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print the questions"
      ],
      "metadata": {
        "id": "tXXAc4kQSXf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, question in enumerate(question_dataset, start=1):\n",
        "    print(f\"{i}. {question}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arZvdpdNhnyx",
        "outputId": "d09dd123-1893-4c10-8b80-fcb2e47e215f"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. What is the high-level design of DevOps pipelines?\n",
            "2. What is a recently introduced feature in Infracost Cloud?\n",
            "3. What is the purpose of Infracost in cloud cost management?\n",
            "4. Why is it important to include TruffleHog in your pipelines?\n",
            "5. How can you fix the vulnerability in the base image according to the provided instructions?\n",
            "6. What is the purpose of the aquasecurity/trivy-action in the GitHub Actions CI workflow?\n",
            "7. What are the optional parameters that can be used with the Checkov action?\n",
            "8. How can Infracost be integrated into the infrastructure pipeline?\n",
            "9. How are application pipelines triggered?\n",
            "10. What is the topic of the second part in the series?\n",
            "11. What command is used to generate the Infracost report in HTML format?\n",
            "12. How does Terraform enable the creation of reusable infrastructure?\n",
            "13. How can the GitHub Actions workflow be configured to dynamically select the backend configuration file based on the environment?\n",
            "14. What is the diff feature in Infracost and how does it serve as a guardrail for cloud cost management?\n",
            "15. How does Infracost provide a safety net for catching abnormal cloud cost estimates?\n",
            "16. What is the purpose of uploading the report to an artifact?\n",
            "17. What types of files or systems can Trivy scan?\n",
            "18. What severity levels does Trivy consider for vulnerabilities?\n",
            "19. What is the purpose of the Terraform GitHub Actions workflow?\n",
            "20. Can you provide a link to a website that provides information on creating Terraform modules?\n",
            "21. What is the intended audience for these documents?\n",
            "22. What is the purpose of the \"DevOps Self-Service -Centric GitHub Actions’ Workflow Orchestration\" article?\n",
            "23. What is the purpose of the \"--soft-fail\" flag in the TFSec step of the workflow?\n",
            "24. What is the topic of the document?\n",
            "25. What command is used to initialize Terraform with a specific backend configuration file?\n",
            "26. What information can Trivy find during a scan?\n",
            "27. What CLI configuration files should be ignored?\n",
            "28. What is the purpose of Terraform as an Infrastructure as Code (IaC) tool?\n",
            "29. What is SonarScan and what does it analyze in source code?\n",
            "30. What are the three samples mentioned in the document?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define jupyter display function\n",
        "def display_eval_df(query: str, response: Response, eval_result: str) -> None:\n",
        "    eval_df = pd.DataFrame(\n",
        "        {\n",
        "            \"Query\": query,\n",
        "            \"Response\": str(response),\n",
        "            \"Source\": response.get_formatted_sources(500) + \"...\",\n",
        "            \"Evaluation Result\": eval_result,\n",
        "        },\n",
        "        index=[0],\n",
        "    )\n",
        "    eval_df = eval_df.style.set_properties(\n",
        "        **{\n",
        "            \"inline-size\": \"600px\",\n",
        "            \"overflow-wrap\": \"break-word\",\n",
        "        },\n",
        "        subset=[\"Response\", \"Source\"]\n",
        "    )\n",
        "    display(eval_df)"
      ],
      "metadata": {
        "id": "7uNjIUK3v7Hj"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Response, test with one question first\n"
      ],
      "metadata": {
        "id": "ew0T81KzSgAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = ResponseEvaluator(service_context=gpt4_service_context)\n",
        "response_vector = query_engine.query(question_dataset[0])\n",
        "eval_result = evaluator.evaluate(response_vector)\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 0)\n",
        "display_eval_df(question_dataset[0], response_vector, eval_result)"
      ],
      "metadata": {
        "id": "YYuUDIeg0b4S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "87b0a564-1f4d-40c2-d62b-8442dcfef09a"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7b2ba08d3dc0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_dca79_row0_col1, #T_dca79_row0_col2 {\n",
              "  inline-size: 600px;\n",
              "  overflow-wrap: break-word;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_dca79\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_dca79_level0_col0\" class=\"col_heading level0 col0\" >Query</th>\n",
              "      <th id=\"T_dca79_level0_col1\" class=\"col_heading level0 col1\" >Response</th>\n",
              "      <th id=\"T_dca79_level0_col2\" class=\"col_heading level0 col2\" >Source</th>\n",
              "      <th id=\"T_dca79_level0_col3\" class=\"col_heading level0 col3\" >Evaluation Result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_dca79_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_dca79_row0_col0\" class=\"data row0 col0\" >What is the high-level design of DevOps pipelines?</td>\n",
              "      <td id=\"T_dca79_row0_col1\" class=\"data row0 col1\" >The high-level design of DevOps pipelines involves two types of pipelines: infrastructure pipelines and application pipelines. The infrastructure pipeline is responsible for provisioning the infrastructure using Terraform, which is an open-source tool for building infrastructure as code. The Terraform GitHub Actions workflow is used to automate the creation of GitHub secrets after successful infrastructure provisioning. These secrets are then used by the application pipelines to kick off CI/CD for the specified GitHub environment. On the other hand, the application pipelines are developed using GitHub Actions and can vary depending on the nature of the applications.</td>\n",
              "      <td id=\"T_dca79_row0_col2\" class=\"data row0 col2\" >> Source (Doc id: e74193a1-280c-4977-b625-e2f79a15b38c): • How do we tie infrastructure pipelines with application pipelines to make \n",
              "them work together seamlessly? We need a glue to integrate these two \n",
              "types of pipelines. And this glue is GitHu b secrets creation automation. \n",
              "Upon successful infrastructure provisioning, we can use Terraform to \n",
              "automate GitHub secrets creation by calling the GitHub provider. Notice \n",
              "the double -ended arrows for the infrastructure pipeline in the diagram \n",
              "above, as the  Terraform outputs for the secrets get insert...\n",
              "\n",
              "> Source (Doc id: 23c321e3-2a5c-4032-a317-b6e3b034f2ac): diagram by author  \n",
              "Note : The diagram does not depict alternative flows, such as for  terraform destroy . \n",
              "You can always add alternative flows per your pipeline requirements.  \n",
              "Application Pipelines  \n",
              "Our application pipelines are developed using GitHub Actions. Below is a high -level \n",
              "overview of the two typical pipelines (CI and CD for microservices). These are mere \n",
              "examples. Your workflows could contain different steps depending on the nature of your \n",
              "applications.  \n",
              "Microservice CI Git......</td>\n",
              "      <td id=\"T_dca79_row0_col3\" class=\"data row0 col3\" >YES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Response for Hallucination\n",
        "\n"
      ],
      "metadata": {
        "id": "oQTLSTsnTDHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def evaluate_query_engine(evaluator, query_engine, questions):\n",
        "    async def run_query(query_engine, q):\n",
        "        try:\n",
        "            return await query_engine.aquery(q)\n",
        "        except:\n",
        "            return Response(response=\"Error, query failed.\")\n",
        "\n",
        "    total_correct = 0\n",
        "    all_results = []\n",
        "    for batch_size in range(0, len(questions), 5):\n",
        "        batch_qs = questions[batch_size:batch_size+5]\n",
        "\n",
        "        tasks = [run_query(query_engine, q) for q in batch_qs]\n",
        "        responses = asyncio.run(asyncio.gather(*tasks))\n",
        "        print(f\"finished batch {(batch_size // 5) + 1} out of {len(questions) // 5}\")\n",
        "\n",
        "        for response in responses:\n",
        "            eval_result = 1 if \"YES\" in evaluator.evaluate(response) else 0\n",
        "            total_correct += eval_result\n",
        "            all_results.append(eval_result)\n",
        "\n",
        "        # helps avoid rate limits\n",
        "        time.sleep(1)\n",
        "\n",
        "    return total_correct, all_results"
      ],
      "metadata": {
        "id": "OmV5PGb4h352"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_correct, all_results = evaluate_query_engine(evaluator, query_engine, question_dataset)\n",
        "\n",
        "print(f\"Hallucination? Scored {total_correct} out of {len(question_dataset)} questions correctly.\")"
      ],
      "metadata": {
        "id": "Djkj0noBv9zC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe9afde-8cc1-41c5-a864-de269d93107d"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished batch 1 out of 6\n",
            "finished batch 2 out of 6\n",
            "finished batch 3 out of 6\n",
            "finished batch 4 out of 6\n",
            "finished batch 5 out of 6\n",
            "finished batch 6 out of 6\n",
            "Hallucination? Scored 27 out of 30 questions correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find out the hallucinated questions and investigate why"
      ],
      "metadata": {
        "id": "xqING50UTLKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "hallucinated_questions = np.array(question_dataset)[np.array(all_results) == 0]\n",
        "print(hallucinated_questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ymOEsDZiNb-",
        "outputId": "13f92c93-e820-4318-aeb7-987129e14b30"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What is the purpose of uploading the report to an artifact?'\n",
            " 'What severity levels does Trivy consider for vulnerabilities?'\n",
            " 'What is the topic of the document?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query('What is the topic of the document?')\n",
        "print(str(response))\n",
        "print(\"-----------------\")\n",
        "print(response.get_formatted_sources(length=1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AURMR784mGyH",
        "outputId": "b8a5fc05-458b-4965-9556-d31ca809f981"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The topic of the document is \"DevOps Self-Service Centric Terraform Project Structure\".\n",
            "-----------------\n",
            "> Source (Doc id: 56053b40-728f-4f95-a1a6-d13cc764a0ee): image by author\n",
            "\n",
            "> Source (Doc id: b13abc3b-9fd7-4670-9320-572d47f211bd): image by author\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Response for Answer Quality"
      ],
      "metadata": {
        "id": "v3kkO5BbTbRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "from llama_index import Response\n",
        "\n",
        "def evaluate_query_engine(evaluator, query_engine, questions):\n",
        "    async def run_query(query_engine, q):\n",
        "        try:\n",
        "            return await query_engine.aquery(q)\n",
        "        except:\n",
        "            return Response(response=\"Error, query failed.\")\n",
        "\n",
        "    total_correct = 0\n",
        "    all_results = []\n",
        "    for batch_size in range(0, len(questions), 5):\n",
        "        batch_qs = questions[batch_size:batch_size+5]\n",
        "\n",
        "        tasks = [run_query(query_engine, q) for q in batch_qs]\n",
        "        responses = asyncio.run(asyncio.gather(*tasks))\n",
        "        print(f\"finished batch {(batch_size // 5) + 1} out of {len(questions) // 5}\")\n",
        "\n",
        "        for question, response in zip(batch_qs, responses):\n",
        "            eval_result = 1 if \"YES\" in evaluator.evaluate(question, response) else 0\n",
        "            total_correct += eval_result\n",
        "            all_results.append(eval_result)\n",
        "\n",
        "        # helps avoid rate limits\n",
        "        time.sleep(1)\n",
        "\n",
        "    return total_correct, all_results"
      ],
      "metadata": {
        "id": "D6b7B7M068pa"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = QueryResponseEvaluator(service_context=gpt4_service_context)\n",
        "\n",
        "total_correct, all_results = evaluate_query_engine(evaluator, query_engine, question_dataset)\n",
        "\n",
        "print(f\"Response satisfies the query? Scored {total_correct} out of {len(question_dataset)} questions correctly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMA1SA-wikT-",
        "outputId": "fcd19a75-4819-423e-ddd8-b8f453bbf226"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished batch 1 out of 6\n",
            "finished batch 2 out of 6\n",
            "finished batch 3 out of 6\n",
            "finished batch 4 out of 6\n",
            "finished batch 5 out of 6\n",
            "finished batch 6 out of 6\n",
            "Response satisfies the query? Scored 23 out of 30 questions correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find out unanswered queries and investigate why"
      ],
      "metadata": {
        "id": "njm5GGw9ThW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "unanswered_queries = np.array(question_dataset)[np.array(all_results) == 0]\n",
        "print(unanswered_queries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7otBiDFioPt",
        "outputId": "2935004b-0b5f-4bb3-dc06-5bb71016c950"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What is a recently introduced feature in Infracost Cloud?'\n",
            " 'What is the topic of the second part in the series?'\n",
            " 'What command is used to generate the Infracost report in HTML format?'\n",
            " 'What is the diff feature in Infracost and how does it serve as a guardrail for cloud cost management?'\n",
            " 'What is the purpose of uploading the report to an artifact?'\n",
            " 'Can you provide a link to a website that provides information on creating Terraform modules?'\n",
            " 'What is the intended audience for these documents?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query('What is the topic of the second part in the series?')\n",
        "print(str(response))\n",
        "print(\"-----------------\")\n",
        "print(response.get_formatted_sources(length=256))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1TTjrGPyOtE",
        "outputId": "54368f5c-c9e0-4db3-da3f-e5a9aa26064b"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The topic of the second part in the series is \"DevOps Self-Service Centric Pipeline Security and Guardrails.\"\n",
            "-----------------\n",
            "> Source (Doc id: 6d5a6daf-46ce-4035-be17-e7421a77f581): DevOps Self -Service Centric GitHub Actions’ Workflow Orchestration  \n",
            "How to orchestrate GitHub Actions’ workflows that a re driven by image immutability  \n",
            "betterprogramming.pub  \n",
            " \n",
            " \n",
            "DevOps Self -Service Centric Pipeline Security and Guardrails  \n",
            "A lis...\n",
            "\n",
            "> Source (Doc id: 892a4b0f-1894-473d-9801-8ecc13db644d): popularity of the Terraform tool and IaC practices to automate deployments in the cloud \n",
            "increasingly.  \n",
            " \n",
            "Image source:  The top programming languages | The State of the Octoverse \n",
            "(github.com)\n"
          ]
        }
      ]
    }
  ]
}